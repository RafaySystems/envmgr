#accounting:
#  enabled: true
#configFiles:
#  gres.conf: |
#    AutoDetect=nvml
#    Name=gpu Type=nvml
accounting:
  slurmdbd:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  initconf:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
#mariadb:
#  primary:
#    persistence:
#      storageClass: ${STORAGECLASS}

controller:
  slurmctld:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
        
  persistence:
    storageClass: ${STORAGECLASS}
    
  reconfigure:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  logfile:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
        
loginsets:
  slinky:
    enabled: true
    replicas: 1
    login:
      image:
        repository: ghcr.io/slinkyproject/login
        tag: 25.05-ubuntu24.04
      env: []
      securityContext:
        privileged: false
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 500m
          memory: 512Mi
      volumeMounts:
        - name: data
          mountPath: /mnt/data
    rootSshAuthorizedKeys: |
      ${SSH_PUB_KEY}
    extraSshdConfig: null
    sssdConf: |
      [sssd]
      config_file_version = 2
      services = nss,pam
      domains = DEFAULT

      [nss]
      filter_groups = root,slurm
      filter_users = root,slurm

      [pam]

      [domain/DEFAULT]
      auth_provider = ldap
      id_provider = ldap
      ldap_uri = ldap://ldap.example.com
      ldap_search_base = dc=example,dc=com
      ldap_user_search_base = ou=Users,dc=example,dc=com
      ldap_group_search_base = ou=Groups,dc=example,dc=com
    metadata: {}
    podSpec:
      initContainers: []
      nodeSelector:
        kubernetes.io/os: linux
      affinity: {}
      tolerations: []
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data

# Slurm NodeSet (slurmd) configurations.
nodesets:
  # Sample NodeSet.
  slinky:
    # -- Enable use of this NodeSet.
    enabled: true
    # -- Number of replicas to deploy.
    replicas: ${COMPUTE_REPLICAS}  
    # slurmd container configurations.
    slurmd:
      # -- The image to use, `${repository}:${tag}`.
      # Ref: https://kubernetes.io/docs/concepts/containers/images/#image-names
      image:
        repository: ${COMPUTE_REPOSITORY}
        tag: ${COMPUTE_TAG}
      # -- Arguments passed to the image.
      # Ref: https://slurm.schedmd.com/slurmd.html#SECTION_OPTIONS
      args: []
        # - -vvv
      # -- The container resource limits and requests.
      # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
      resources:
        limits:
          cpu: ${CPUS}
          memory: ${MEMORY}
          nvidia.com/gpu: ${GPUS}
      volumeMounts:
        - name: data
          mountPath: /mnt/data
    # LogFile sidecar configurations.
    logfile:
      # -- The image to use, `${repository}:${tag}`.
      # Ref: https://kubernetes.io/docs/concepts/containers/images/#image-names
      image:
        repository: docker.io/library/alpine
        tag: latest
      # -- The container resource limits and requests.
      # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi
    # -- Extra configuration added to the `--conf` argument.
    # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_NODE-CONFIGURATION
    extraConf: null
    # -- (map[string]string \| map[string][]string) Extra configuration added to the `--conf` argument.
    # If `extraConf` is not empty, it takes precedence.
    # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_NODE-CONFIGURATION
    extraConfMap: {}
      # Features: []
      # Gres: []
      # Weight: 1
    # Partition configuration for this NodeSet.
    partition:
      # -- Enable NodeSet partition creation.
      enabled: true
      # -- The Slurm partition configuration options added to the partition line added to the partition line.
      # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_PARTITION-CONFIGURATION
      config: null
      # -- (map[string]string \| map[string][]string) The Slurm partition configuration options added to the partition line.
      # If `config` is not empty, it takes precedence.
      # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_PARTITION-CONFIGURATION
      configMap: {}
        # State: UP
        # MaxTime: UNLIMITED
    # -- Enable propagation of container `resources.limits` into slurmd.
    useResourceLimits: true
    # -- Labels and annotations.
    # Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    metadata: {}
      # annotations: {}
      # labels: {}
    # -- (corev1.PodSpec) Extend the pod template, and/or override certain configurations.
    # Ref: https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates
    podSpec:
      # -- Additional initContainers for the pod.
      # Ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
      # Ref: https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/
      initContainers: []
      # -- (map[string]string) Node label selector for pod assignment.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
${NODE_SELECTOR_BLOCK}
      # -- Affinity for pod assignment.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
      affinity: {}
      # -- Tolerations for pod assignment.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
      tolerations: []
        # - key: nvidia.com/gpu
        #   effect: NoSchedule
      # -- List of volumes to use.
      # Ref: https://kubernetes.io/docs/concepts/storage/volumes/
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data

slurm-exporter:
  enabled: true
  exporter:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi



restapi:
  slurmrestd:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi