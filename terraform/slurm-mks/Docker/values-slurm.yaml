configFiles:
  plugstack.conf: |
    include /usr/share/pyxis/*

accounting:
  podSpec:
${NODE_SELECTOR_BLOCK_4SPACE}
  slurmdbd:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  initconf:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
#mariadb:
#  primary:
#    persistence:
#      storageClass: ${STORAGECLASS}

controller:
  podSpec:
${NODE_SELECTOR_BLOCK_4SPACE}
  slurmctld:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
        
  persistence:
    storageClass: ${STORAGECLASS}
    
  reconfigure:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  logfile:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
        
loginsets:
  slinky:
    enabled: true
    replicas: 1
    login:
      image:
        repository: ghcr.io/slinkyproject/login-pyxis
        tag: 25.11.0-ubuntu24.04
      env: []
      securityContext:
        privileged: true
        runAsUser: 0
        runAsGroup: 0
        fsGroup: 401
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 500m
          memory: 512Mi
      volumeMounts:
        - name: data
          mountPath: /mnt/data
        - name: users
          mountPath: /mnt/users
      lifecycle:
        postStart:
          exec:
            command:
              - sh
              - -c
              - |
                echo "[INFO] === Starting postStart setup ==="

                # Ensure PVC directories exist
                chmod 700 /mnt/users

                chown root:401 /mnt/data
                chmod 770 /mnt/data

                echo "[INFO] Directories prepared."
                
                echo "[INFO] Merging PVC users into system..."
                for file in passwd group shadow; do
                  if [ -f /mnt/users/$file ]; then
                    echo "[INFO] Merging /mnt/users/$file -> /etc/$file"
                    grep -v '^#' /mnt/users/$file >> /etc/$file
                  fi
                done

                # Ensure permissions are correct
                chmod 644 /etc/passwd /etc/group
                chmod 600 /etc/shadow

                # Create home directories if missing
                if [ -f /mnt/users/passwd ]; then
                  awk -F: '{print $6}' /mnt/users/passwd | while read home; do
                    [ -n "$home" ] && mkdir -p "$home" && chmod 700 "$home"
                  done
                fi

                echo "[INFO] User merge complete."



    rootSshAuthorizedKeys: |
      ${SSH_PUB_KEY}
    extraSshdConfig: |
      PasswordAuthentication yes
      UsePAM yes
    sssdConf: |
      [sssd]
      config_file_version = 2
      services = nss,pam
      domains = localusers

      [nss]
      filter_groups = root,slurm
      filter_users = root,slurm

      [pam]

      [domain/localusers]
      id_provider = files
      auth_provider = files
      enumerate = true
      cache_credentials = false
      override_homedir = /home/%u
    metadata: {}
    podSpec:
${NODE_SELECTOR_BLOCK_6SPACE}
      affinity: {}
      tolerations: []
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data
        - name: users
          persistentVolumeClaim:
            claimName: users        
        
# Slurm NodeSet (slurmd) configurations.
nodesets:
  # Sample NodeSet.
  slinky:
    updateStrategy:
      # -- The strategy type. Can be one of: RollingUpdate; OnDelete.
      type: RollingUpdate
      # The RollingUpdate configuration. Ignored unless `type=RollingUpdate`.
      rollingUpdate:
        # -- Maximum number of pods that can be unavailable during update.
        # Can be an absolute number (ex: 5) or a percentage (ex: 25%).
        maxUnavailable: 25%
    # -- Enable use of this NodeSet.
    enabled: true
    # -- Number of replicas to deploy.
    replicas: ${COMPUTE_REPLICAS}  
    # slurmd container configurations.
    slurmd:
      # -- The image to use, `${repository}:${tag}`.
      # Ref: https://kubernetes.io/docs/concepts/containers/images/#image-names
      image:
        repository: ${COMPUTE_REPOSITORY}
        tag: ${COMPUTE_TAG}
      # -- Arguments passed to the image.
      # Ref: https://slurm.schedmd.com/slurmd.html#SECTION_OPTIONS
      args: []
        # - -vvv
      # -- The container resource limits and requests.
      # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
      resources:
        limits:
          #cpu: ${CPUS}
          #memory: ${MEMORY}
          nvidia.com/gpu: ${GPUS}
${VOLUME_MOUNTS_BLOCK}
    # LogFile sidecar configurations.
    logfile:
      # -- The image to use, `${repository}:${tag}`.
      # Ref: https://kubernetes.io/docs/concepts/containers/images/#image-names
      image:
        repository: docker.io/library/alpine
        tag: latest
      # -- The container resource limits and requests.
      # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi
    # -- Extra configuration added to the `--conf` argument.
    # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_NODE-CONFIGURATION
    extraConf: null
    # -- (map[string]string \| map[string][]string) Extra configuration added to the `--conf` argument.
    # If `extraConf` is not empty, it takes precedence.
    # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_NODE-CONFIGURATION
    extraConfMap: {}
      # Features: []
      # Gres: []
      # Weight: 1
    # Partition configuration for this NodeSet.
    partition:
      # -- Enable NodeSet partition creation.
      enabled: true
      # -- The Slurm partition configuration options added to the partition line added to the partition line.
      # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_PARTITION-CONFIGURATION
      config: null
      # -- (map[string]string \| map[string][]string) The Slurm partition configuration options added to the partition line.
      # If `config` is not empty, it takes precedence.
      # Ref: https://slurm.schedmd.com/slurm.conf.html#SECTION_PARTITION-CONFIGURATION
      configMap: {}
        # State: UP
        # MaxTime: UNLIMITED
    # -- Enable propagation of container `resources.limits` into slurmd.
    useResourceLimits: true
    # -- Labels and annotations.
    # Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    metadata: {}
      # annotations: {}
      # labels: {}
    # -- (corev1.PodSpec) Extend the pod template, and/or override certain configurations.
    # Ref: https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates
    podSpec:
      # -- Additional initContainers for the pod.
      # Ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
      # Ref: https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/
      initContainers: []
      # -- (map[string]string) Node label selector for pod assignment.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
${NODE_SELECTOR_BLOCK_6SPACE}
      # -- Affinity for pod assignment.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
      affinity: {}
      # -- Tolerations for pod assignment.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
      tolerations: []
${VOLUMES_BLOCK}
  pyxis:
    updateStrategy:
      # -- The strategy type. Can be one of: RollingUpdate; OnDelete.
      type: RollingUpdate
      # The RollingUpdate configuration. Ignored unless `type=RollingUpdate`.
      rollingUpdate:
        # -- Maximum number of pods that can be unavailable during update.
        # Can be an absolute number (ex: 5) or a percentage (ex: 25%).
        maxUnavailable: 25%
    enabled: true
    replicas: ${COMPUTE_REPLICAS}  
    slurmd:
      image:
        repository: ghcr.io/slinkyproject/slurmd-pyxis
        tag: 25.05.4-ubuntu24.04
      args: []
      resources:
        limits:
          #cpu: ${CPUS}
          #memory: ${MEMORY}
          nvidia.com/gpu: ${GPUS}
${VOLUME_MOUNTS_BLOCK}
    logfile:
      image:
        repository: docker.io/library/alpine
        tag: latest
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi
    extraConf: null
    extraConfMap: {}
    partition:
      enabled: true
      config: null
      configMap: {}
    useResourceLimits: true
    metadata: {}
    podSpec:
      initContainers: []
${NODE_SELECTOR_BLOCK_6SPACE}
      affinity: {}
      tolerations: []
${VOLUMES_BLOCK}


slurm-exporter:
  enabled: true
  exporter:
${NODE_SELECTOR_BLOCK_4SPACE}
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi



restapi:
  podSpec:
${NODE_SELECTOR_BLOCK_4SPACE}
  slurmrestd:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

${PROLOG_BLOCK}
${EPILOG_BLOCK}