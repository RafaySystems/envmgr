{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e65306",
   "metadata": {},
   "source": [
    "# Building a RAG application using Rafay Environment Manager and Jupyter Notebooks\n",
    "\n",
    "The document describes how to implement a Retriever-Augmented Generation (RAG) pipeline in a Jupyter notebook setting. It utilizes LangChain to build the RAG pipeline, which is designed to answer questions based on the `Attention is all you need paper`` from Arxiv. The AWS Bedrock Large Language Model (LLM) is employed in conjunction with the Qdrant vector database for text-generation and document embeddings storage.\n",
    "\n",
    "The infrastructure is provisioned using [Rafay Environment Manager](https://docs.rafay.co/env_manager/overview/). Rafay Environment Manager simplifies the process of setting up and managing cloud-based environments necessary for deploying sophisticated AI applications. It provides a streamlined approach for Development and DevOps teams, allowing them to quickly and efficiently create environments tailored for specific needs. At the same time, it ensures that Ops, SRE, and Platform teams have the necessary controls to enforce security, cost-efficiency, governance, and standardization. For more information refer to [docs](https://docs.rafay.co/env_manager/overview/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513119d2",
   "metadata": {},
   "source": [
    "Install the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd64d28-de22-4f0b-961b-059ed7e38932",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 langchain openai PyPDF2 pypdf qdrant-client tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870bec6d-43c6-4656-8465-e23c6604fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import boto3\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a28172",
   "metadata": {},
   "source": [
    "Configure environment variables. Rafay environment manager populates these variables for you, this can be configured from environment manager application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bc181",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_url = os.environ.get('QDRANT_URL')\n",
    "open_ai_secret_prefix = os.environ.get('OPEN_AI_SECRET_PREFIX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918469b9",
   "metadata": {},
   "source": [
    "Helper functions for fetching the secrets from `AWS Secrets Manager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986de38b-e210-4718-86a4-6774299a3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_secret(secret_prefix):\n",
    "    client = boto3.client('secretsmanager')\n",
    "    secret_arn = locate_secret_arn(secret_prefix, client)\n",
    "    secret_value = client.get_secret_value(SecretId=secret_arn)\n",
    "    return secret_value['SecretString']\n",
    "def locate_secret_arn(secret_tag_value, client):\n",
    "    response = client.list_secrets(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Key': 'tag-key',\n",
    "                'Values': ['Name']\n",
    "            },\n",
    "            {\n",
    "                'Key': 'tag-value',\n",
    "                'Values': [secret_tag_value]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response['SecretList'][0]['ARN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce077bb",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Load the pdf document, extract the content from the PDF documents and split it into small chunks. OpenAI embedding model is used to compute the embeddings for each chunk and then stored to Qdrant vector database. In the example below we ingest `Attention is all you need` paper from ArXiv - https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84682937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF document loading\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")\n",
    "\n",
    "# Load and split the document into small chunks.\n",
    "docs = loader.load_and_split() \n",
    "\n",
    "# Using openai api compute embeddings for all the text chunks\n",
    "open_ai_key = get_secret(open_ai_secret_prefix)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=open_ai_key)\n",
    "\n",
    "# Store embeddings into the Qdrant vector database.\n",
    "qdrant = Qdrant.from_documents(docs,embeddings,url=qdrant_url, collection_name='rag', prefer_grpc=True)\n",
    "print(\"Stored documents sucessfully to qdrant collection \" + qdrant.collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a76485",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Initiate the retrieval process by querying the AWS Bedrock LLM, which leverages advanced language models for understanding and interpreting the context of the questions posed. The system utilizes LangChain's QA chain to efficiently map queries to relevant information extracted from PDF documents.\n",
    "This integration ensures precise extraction of information, with the LangChain QA chain effectively pinpointing the most relevant data chunks stored in the Qdrant vector database. The retrieved information is then presented in a user-friendly format, providing accurate and contextually relevant answers to the posed queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c758d0-9f7c-4f97-86d3-a3e2d0121862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval configuration\n",
    "retrieve_top_k = 3\n",
    "temperature = 0.5\n",
    "max_tokens = 200\n",
    "retrieve_top_p = 0.5\n",
    "\n",
    "\n",
    "rag_query = \"What are transformers?\"\n",
    "\n",
    "# Compute the embedding for rag_query\n",
    "open_ai_key = get_secret(open_ai_secret_prefix)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=open_ai_key)\n",
    "\n",
    "# Load the bedrock llm\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "bedrock_llm = Bedrock(\n",
    "    model_id=\"ai21.j2-ultra-v1\",\n",
    "    client=bedrock_client,\n",
    "    model_kwargs={'temperature': temperature, 'maxTokens': max_tokens, 'topP':retrieve_top_p}\n",
    ")\n",
    "\n",
    "# Retrive the top k documents similar to the provided rag_query\n",
    "client = QdrantClient(url=qdrant_url, prefer_grpc=True)\n",
    "qdrant = Qdrant(client=client, collection_name='rag', embeddings=embeddings)\n",
    "search_results = qdrant.similarity_search(rag_query, k=retrieve_top_k)\n",
    "\n",
    "# Generate the response and present it to the user.\n",
    "chain = load_qa_chain(bedrock_llm,chain_type=\"stuff\")\n",
    "results = chain({\"input_documents\": search_results, \"question\": rag_query}, return_only_outputs=False)\n",
    "print(f\"Question:\\n {results['question']} \\n\")\n",
    "print(f\"Answer:\\n {results['output_text']} \\n\")\n",
    "print(f\"Sources:\\n\")\n",
    "print(f\"------------------------------------\\n\")\n",
    "for doc in results[\"input_documents\"]:\n",
    "    print(f\"Page No: {doc.metadata['page']} \\n\")\n",
    "    print(f\"Source: {doc.metadata['source']} \\n\")\n",
    "    print(f\"Page Content: \\n {doc.page_content} \\n\")\n",
    "    print(f\"------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
